<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>PCA</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    R
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="pivotwider.html">tidyr</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Machine Learning
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="regularized.html">Regularized Linear Models</a>
    </li>
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">Trees</a>
      <ul class="dropdown-menu" role="menu">
        <li>
          <a href="trees.html">Decision Trees Concept</a>
        </li>
        <li>
          <a href="classtree.html">Classification Trees</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="pca.html">PCA</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ci.html">Misconceptions About CIs</a>
    </li>
    <li>
      <a href="pvalue.html">P-value</a>
    </li>
    <li>
      <a href="probvslike.html">Probability vs Likelihood</a>
    </li>
    <li>
      <a href="mle.html">MLE</a>
    </li>
    <li>
      <a href="clt.html">Central Limit Theorem</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">PCA</h1>

</div>


<style type="text/css">
body {
  font-family: arial;
  font-size: 13pt;
  color: black;
}
</style>
<style> body {text-align: justify} </style>
<p>Principal Component Analysis (PCA) is one of the famous
dimensionality reduction methods. When your model has too many
variables, PCA helps narrowing them down to two that best explains the
model. Let’s walk through the process one-by-one with a model that has
two variables.</p>
<p>PCA starts by computing the average (red X in plot 1) of each
variable. Then we can plot the center of the data by using these
averages:</p>
<p><img src="pca_files/figure-html/unnamed-chunk-2-7.png" width="672" /></p>
<p>Next, we shift the data such that the center (marked as red) is at
the origin. Note that the relative positions have not changed.</p>
<p><img src="pca_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We now find the best line that fits the samples given that the line
passes the origin.</p>
<p><img src="pca_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>To tell how good the fit is PCA projects the samples onto the line
and then either find the line that minimizes the distances from the
samples to the line <strong>or</strong> maximizes the distances from the
projected points to the origin. Although intuitively it makes sense to
go with the former, PCA prefers the latter as the computation is easier.
We quantify the distances by adding up the squared distances between the
projected points to the origin i.e., <strong>sum of squared
distances</strong>. The line with the largest <strong>sum of squared
distances</strong> is called a <strong>Principal Component</strong>
(PC). We call the sum of squared distances for PC1 the
<strong>Eigenvalue</strong> and the square root the <strong>Singular
Value</strong></p>
<p><img src="pca_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Note the slope is about .30. This means for every 10 units we go
along the x axis we go up 3 units along the y axis. So most of our data
is spread out along the x axis, hence our x variable is more important
when it comes to explaining how the data are spread out. We call this a
linear combination of x and y.</p>
<p>Instead of using the original slope, we change the x and y values by
making a unit vector (divide by the slope) so that the length of the
slope becomes 1. We call this unit vector the
<strong>Eigenvector</strong> or <strong>Singular Vector</strong> of
<strong>PC1</strong> and call the proportions of x and y <strong>Loading
Scores</strong>.</p>
<p>To get PC2, we simply draw a line perpendicular to PC1. Then like we
did for PC1, we scale to get a unit vector which becomes the Eigenvector
for PC2.</p>
<p><img src="pca_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>To plot the final PCA plot, we rotate so that PC1 is horizontal and
then we use the projected points to find the positions of the data
points. This is how PCA is performed using Singular Value Decomposition
(SVD).</p>
<p><img src="pca_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Lastly, we can convert the eigenvalues of PC1 and PC2 into variation
around the origin by dividing by the sample size minus 1. Using the
<code>summary</code>method in R, PCA tells us that our PC1 accounts for
82% of variation.</p>
<pre class="r"><code>pca &lt;- princomp(df)
# loadings(pca)
summary(pca)</code></pre>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2
## Standard deviation     0.8682098 0.4008325
## Proportion of Variance 0.8243033 0.1756967
## Cumulative Proportion  0.8243033 1.0000000</code></pre>
<p>Now we will learn how to perform PCA in Python.</p>
<pre class="python"><code>## Python 3.9.6
import pandas as pd
import numpy as np
import random as rd
from sklearn.decomposition import PCA
from sklearn import preprocessing
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn import datasets
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
np.set_printoptions(suppress=True)</code></pre>
<p>We will make a toy dataset from a poisson distribution with rows
being 10 different customers and columns being 5 different types</p>
<pre class="python"><code>customer = [&#39;customer&#39; + str(i) for i in range(1, 11)]
type = [&#39;type&#39; + str(i) for i in range(1, 6)]
data = pd.DataFrame(columns = [*type], index=customer)
data.shape # (10, 5)</code></pre>
<pre><code>## (10, 5)</code></pre>
<pre class="python"><code>np.random.seed(0)
for row in data.index:
  data.loc[row, &#39;type1&#39;:&#39;type5&#39;] = np.random.poisson(rd.randrange(10, 1000), 5)

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Fit
pca = PCA() # returns the same shape as the original data i.e. in our case 5 dimensions as we have 5 variables
# pca = PCA(n_components = 2) # returns 2 principal components
# pca = PCA(.95) # chooses the minimum number of principal components such that 95% of the variance is retained
pca.fit(scaled_data) # computes loading scores and variation for each principle component</code></pre>
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>PCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">PCA</label><div class="sk-toggleable__content"><pre>PCA()</pre></div></div></div></div></div>
<pre class="python"><code>pca_data = pca.transform(scaled_data) # generate coordinates to plot PCA graph based on the loading scores and the scaled data
pca_data.shape # (10, 5) =&gt; returns the same shape as the original data if nothing is specified in PCA()

# scree plot *</code></pre>
<pre><code>## (10, 5)</code></pre>
<pre class="python"><code>var_percentage = np.around(pca.explained_variance_ratio_ * 100, 2)
labels = [&#39;PC&#39; + str(x) for x in range(1, len(var_percentage) + 1)]

plt.bar(x = range(1, len(var_percentage) + 1), height = var_percentage, tick_label = labels)</code></pre>
<pre><code>## &lt;BarContainer object of 5 artists&gt;</code></pre>
<pre class="python"><code>plt.xlabel(&quot;PC&quot;)
plt.ylabel(&quot;Variance %&quot;)
plt.title(&quot;Scree Plot&quot;)
plt.show()</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="python"><code>pca_df = pd.DataFrame(pca_data, index=[*customer], columns = labels)
pca_df.shape # (10, 5)

# PCA Plot</code></pre>
<pre><code>## (10, 5)</code></pre>
<pre class="python"><code>plt.scatter(pca_df[&#39;PC1&#39;], pca_df[&#39;PC2&#39;])
plt.title(&quot;PCA Plot&quot;)
plt.xlabel(f&#39;PC1 {var_percentage[0]} %&#39;)
plt.ylabel(f&#39;PC 2 {var_percentage[1]}%&#39;)
for i in pca_df.index:
  print(i)
  plt.annotate(i, (pca_df.PC1.loc[i], pca_df.PC2.loc[i]))
plt.show()</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<p>Now We fit PCA using another dataset <code>iris</code> from Michael
Galarnyk’s Medium post. The process is the same as before but we have 3
target labels for this case</p>
<pre class="python"><code>iris = datasets.load_iris()
iris_df = pd.DataFrame(iris.data)
iris_df[&#39;target&#39;] = iris.target

features = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_lenth&#39;, &#39;petal_width&#39;]
iris_df.columns = features + [&#39;target&#39;]

X = iris_df.loc[:, features].values # values: make iris_df np.array
y = iris_df.loc[:, &#39;target&#39;].values

X_scaled = StandardScaler().fit_transform(X)

# Projects and thus reduces 4D into 2D. No special meaning attached for principal components as they are just the two main dimensions of variation

# First fit with all 4 variables
pca = PCA()
pca_data = pca.fit_transform(X_scaled)
pca_data.shape # 4D

# scree plot *</code></pre>
<pre><code>## (150, 4)</code></pre>
<pre class="python"><code>var_percentage = np.around(pca.explained_variance_ratio_ * 100, 2) # *
labels = [&#39;PC&#39; + str(x) for x in range(1, len(var_percentage) + 1)]

plt.bar(x = range(1, len(var_percentage) + 1), height = var_percentage, tick_label = labels)</code></pre>
<pre><code>## &lt;BarContainer object of 4 artists&gt;</code></pre>
<pre class="python"><code>plt.show()</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-11-5.png" width="672" /></p>
<pre class="python"><code>plt.title(&quot;Scree Plot&quot;)
plt.xlabel(&quot;PC&quot;)
plt.ylabel(&quot;Variance %&quot;)
plt.show()

# Scree plot tells us that PC1 and PC2 explains most of the variance of our dataset, hence refit with 2 PCs</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-11-6.png" width="672" /></p>
<pre class="python"><code>pca = PCA(n_components = 2)
pca_data = pca.fit_transform(X_scaled)
pca_data.shape[1] # 2D</code></pre>
<pre><code>## 2</code></pre>
<pre class="python"><code>labels = [&#39;PC&#39; + str(x) for x in range(1, len(var_percentage) + 1)]

pca_df = pd.DataFrame(data = pca_data, columns = [&#39;PC&#39; + str(x) for x in range(1, pca_data.shape[1] + 1)])

pca_df_with_target = pd.concat([pca_df, iris_df[&#39;target&#39;]], axis = 1)

targets = [0, 1, 2]
colors = [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;]

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel(&#39;PC 1&#39;, fontsize = 15)
ax.set_ylabel(&#39;PC 2&#39;, fontsize = 15)
ax.set_title(&#39;PCA Plot&#39;, fontsize = 20)
for target, color in zip(targets, colors):
  indicesToKeep = pca_df_with_target[&#39;target&#39;] == target # grabs three target values individually as the target variable is inside a for loop of targets variable -&gt; then will plot ax.scatter individually, hence the for loop acts as a group by
  ax.scatter(
    pca_df_with_target.loc[indicesToKeep, &#39;PC1&#39;],
    pca_df_with_target.loc[indicesToKeep, &#39;PC2&#39;], 
    c = color, 
    s = 50
    )
ax.legend(targets)
plt.show()</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-11-7.png" width="768" /></p>
<p>One can also us PCA to speed up an ML algorithm such as logistic
regression after reducing dimensionality using PCA. We started with 784
features from the MNIST dataset that was reduced to 327 principal
components which then can be fed for other ML models.</p>
<pre class="python"><code># mnist = fetch_openml(&#39;mnist_784&#39;)
# 
# X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=1/7.0, random_state=0)
# X_train.shape # 60,000 images with 784 features each
# 
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# 
# pca = PCA(.95) # Chooses the minimum number of PCs such that 95% of variance is retained
# 
# pca_data = pca.fit_transform(X_train_scaled)
# pca.n_components_ # 327 PCs chosen
# pca_data.shape # (60000, 327)</code></pre>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
